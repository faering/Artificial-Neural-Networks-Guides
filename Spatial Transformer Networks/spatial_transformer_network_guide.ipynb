{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Transformer Network\n",
    "This is a guide for creating a Convolutional Neural Network with a Spatial Transformer Network (STN) for **image classification**. <br>\n",
    "The guide is for personal use as a reference for future work regarding image classification problems.\n",
    "\n",
    "The guide is heavily influenced by the official PyTorch tutorial found at https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html and the STN is first proposed by Google DeepMind in this paper https://arxiv.org/abs/1506.02025.\n",
    "\n",
    "STNs is a visual attention mechanism that allow ANNs to learn to perform spatial transformation to images, to enhance the geometric invariance of the model. This is useful for CNNs as they are not affected by transformation of the images as known in image augmentaion.\n",
    "\n",
    "The STN architecture consists of three main components:\n",
    "- A **localization network** which is a regular CNN that finds the transformation parameters.\n",
    "- A **grid generator** which generates a set of coordinates in the input image.\n",
    "- A **sampler** which uses the transformation parameters and applies them to the input image.\n",
    "\n",
    "The figure below shows the architecture of a SPT network.\n",
    "<img src=\"./static_files/stn-architecture.png\"/>\n",
    "\n",
    "In this guide we will use the CIFAR-10 dataset. The steps of the guide are as follows:\n",
    "1. Load the CIFAR-10 dataset using *torchvision*\n",
    "2. Define a *Convolutional Neural Network*\n",
    "3. Define a loss function for the CNN\n",
    "4. Train the network\n",
    "5. Test the network\n",
    "6. Visualize the STN results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "The CIFAR-10 dataset consists images of the size 3x32x32, which means they are RGB color images with 3-channels and are of the size 32x32. There are ten different classes in the dataset as seen in the figure below.\n",
    "\n",
    "<img src=\"./static_files/cifar10.png\"/>\n",
    "\n",
    "We will load the dataset easily using ``torchvision``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defines\n",
    "N_CHANNELS = 3\n",
    "DIM_HEIGHT = DIM_WIDTH = 32\n",
    "N_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "We transform them to Tensors of normalized range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                        train=True,\n",
    "                                        download=True, \n",
    "                                        transform=transforms.Compose(\n",
    "                                            [transforms.ToTensor(),\n",
    "                                             transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                                  (0.5, 0.5, 0.5))]\n",
    "                                        ))\n",
    "\n",
    "# Test dataset\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                       train=False,\n",
    "                                       download=True, \n",
    "                                       transform=transforms.Compose(\n",
    "                                           [transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                                 (0.5, 0.5, 0.5))]\n",
    "                                       ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "(samples, height, width, channels)\n",
      "(50000, 32, 32, 3)\n",
      "\n",
      "Test data\n",
      "(samples, height, width, channels)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data\\n(samples, height, width, channels)\")\n",
    "print(trainset.train_data.shape)\n",
    "\n",
    "\n",
    "print(\"\\nTest data\\n(samples, height, width, channels)\")\n",
    "print(testset.test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(3, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(8, 12, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace)\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=192, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Dropout2d(p=0.2)\n",
      "    (3): ReLU(inplace)\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=50, bias=True)\n",
      "    (1): Dropout2d(p=0.2)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (fc_out): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=10, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, input_height, input_width, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.input_height = input_height\n",
    "        self.input_width = input_width\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_channels,\n",
    "                out_channels=8,\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=3),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=8,\n",
    "                out_channels=12,\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=2),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # regressor for the 3 * 2 affine matrix that we use \n",
    "        # to make the bilinear interpolation for the spatial transformer\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=input_height//4 * input_width//4 * input_channels,\n",
    "                out_features=32,\n",
    "                bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(\n",
    "                in_features=32,\n",
    "                out_features=3 * 2,\n",
    "                bias=True))\n",
    "        \n",
    "        ## network for the classification based on the transformed input image\n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, \n",
    "                      out_channels=16, \n",
    "                      kernel_size=5, \n",
    "                      stride=1, \n",
    "                      padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2),\n",
    "            nn.ReLU(inplace=True))\n",
    "            \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                      out_channels=32,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=2048,\n",
    "                      out_features=50,\n",
    "                      bias=True),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(in_features=50,\n",
    "                      out_features=num_classes,\n",
    "                      bias=False))\n",
    "        \n",
    "        self.fc_loc[2].weight.data.fill_(0)\n",
    "        self.fc_loc[2].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0])\n",
    "        \n",
    "        \n",
    "    def stn(self, x):\n",
    "        \"\"\" Spatial Transformer Network \"\"\"\n",
    "        \n",
    "        # find the transformation parameters\n",
    "        xs = self.localization(x)\n",
    "\n",
    "        # transformation tensor must be same size as in_features of regressor (fc_loc)\n",
    "        # notice the channels of the image are important\n",
    "        # and multiplying with a factor of 10 is only to increase the number of features in the first layer\n",
    "        xs = xs.view(-1, self.input_height//4 * self.input_width//4 * self.input_channels) \n",
    "        \n",
    "        # input batch of affine matrices (N x 2 x 3)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3) # the size -1 (N) is inferred from other dimensions\n",
    "\n",
    "        # generate coordinates for the input image\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        \n",
    "        # apply transformation parameters to input iamge\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward Pass \"\"\"\n",
    "        # perform transformation of input image\n",
    "        x = self.stn(x)\n",
    "\n",
    "        # usual forward pass - classification network\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        print(x.size())\n",
    "        x = x.view(-1, 2048)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "net = Net(N_CHANNELS, DIM_HEIGHT, DIM_WIDTH, N_CLASSES).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Test Forward Pass\n",
    "We test the networks forward pass on dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 32, 8, 8])\n",
      "[torch.Size([10]), torch.Size([10]), torch.Size([10]), torch.Size([10]), torch.Size([10])]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0, 1, (5, 3, 32, 32)).astype('float32')\n",
    "x = torch.autograd.Variable(torch.from_numpy(x)).to(device)\n",
    "output = net(x)\n",
    "print([x.size() for x in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
